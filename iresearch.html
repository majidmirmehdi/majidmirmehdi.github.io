
<!DOCTYPE html>
<html>
<head>
<title>Professor Majid Mirmehdi</title>

<style>
		.container {
			position: relative;
		}
		.left-table {
			display: inline-block;
			vertical-align: top;
			margin: 10px;
		}
		.right-table {
			position: absolute;
			top: 0;
			right: 0;
			margin: 10px;
		}
	</style>

<style>
body {
  margin: 0;
  font-family: Arial, Helvetica, sans-serif;
}

.topnav {
  overflow: auto;
  background-color: #ccc;
}

.topnav a {
  float: left;
  color: #222222;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
  font-size: 17px;
}

.topnav a:hover {
  background-color: #eee;
  color: black;
}

</style>
</head>

<body>



<div class="topnav">
  <a class="active" href="index.html">Home</a>
  <a href="iresearch.html">Research</a>
  <a href="https://scholar.google.co.uk/citations?user=NsW3yAwAAAAJ&hl=en">Publications</a>
  <a href="iteach.html">Teaching</a>
  <a href="iact.html">Activities</a>
  <a href="iteam.html">RAs & Students</a>
  <a href="https://uob-mavi.github.io/people/">MAVI</a>
</div>




<div class="container">
<table class="left-table">
<TR>
<TD> <IMG SRC="./MMphoto.jpg" width=300 ALT="Majid Mirmehdi", ALIGN=LEFT>
<TD>
<dd><h2>Prof. Majid Mirmehdi</h2>
<h3>Professor of Computer Vision</h3>
<dd><h3>Fellow IAPR, BMVA Distinguished Fellow</h3>
<dd><a href="https://uob-mavi.github.io/people/">Machine Learning and Computer Vision</a>
<dd><a href="http://www.cs.bris.ac.uk/">Department of Computer Science</a>
<dd><a href="http://www.bris.ac.uk/"> University of Bristol</a>
<dd><a href="http://www.cs.bris.ac.uk/Research/Vision/mvb.html">Merchant Venturers Building</a>
<dd>Woodland Road, <a href="http://www.brisindex.co.uk/">Bristol</a> BS8 1UB, <a href="http://www.uktravel.com/index.html">UK</a>
<dd>Tel: +44 (0)117-455 8158
<dd>Email: <a href="mailto:M.Mirmehdi@cs.bris.ac.uk">M.Mirmehdi@cs.bris.ac.uk</a> </b>
<dd>ORCID:<a href="https://orcid.org/0000-0002-6478-1403">0000-0002-6478-1403</a> </b>
<p>
<!--
 <a class="twitter-follow-button" href="http://twitter.com/majidmirmehdi?ref_src=twsrc%5Etfw%20-filter%3Aretweets%20-filter%3Amentions" data-size="large"> Follow @majidmirmehdi</a>
-->
 <TD> <a href="http://www.iapr.org"><img src="./IAPRLogo_fellowRev.gif"  alt="IAPR Fellow" border="0" height="90"  width="180" align="right"></a>

</TR>
</table>




<table class="right-table">
<tr>
<TD valign="top">
<!-- <a class="twitter-timeline" data-width="400" data-height="650" href="https://twitter.com/majidmirmehdi">Tweets by Majid Mirmehdi</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
-->
</td>
</tr>
</table>

<table>
<TD>

<hr size="1">
   <a href="https://torus.ac.uk/">TORUS</a> | <a href="https://wilddrone.eu/">Wilddrone</a> |  <a href="http://irc-sphere.ac.uk">SPHERE</a> | PD Sensors |


 <hr size="1">
    <h2>Selection of Publications</h2>

		<TABLE>
		<TD>  <img src="./PANAfFGBG.png", width="160", height="120" , ALIGN=LEFT/></a>
		<TD>
		<DL>
			<DD> The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition. O Brookes, M Kukushkin, M Mirmehdi, C Stephens, P Dieguez, TC Hicks, S Jones, K Lee, MS McCarthy, A Meier, E Normand, EG Wessling, RM Wittig, K Langergraber, K Zuberbuehler, L Boesch, T Schmid, M Arandjelovic, H Kuehl, T Burghardt. <em>38th IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2025. (<a href="https://arxiv.org/pdf/2502.21201" target="_blank">Paper PDF</a>)
		</DL>
		</TABLE>


		  <TABLE>
		  <TD>  <img src="./panaf.png", width="160", height="120" , ALIGN=LEFT/></a>
		  <TD>
		  <DL>
				<DD> PanAf20K: A Large Video Dataset for Wild Ape Detection and Behaviour Recognition. O Brookes, M Mirmehdi, C Stephens, S Angedakin, K Corogenes, D Dowd, P Dieguez, TC Hicks, S Jones, K Lee, V Leinert, J Lapuente, MS McCarthy, A Meier, M Murai, E Normand, V Vergnes, EG Wessling, RM Wittig, K Langergraber, N Maldonado, X Yang, K Zuberbuhler, C Boesch, M Arandjelovic, H Kuhl, T Burghardt. <em>International Journal of Computer Vision (IJCV).</em> March 2024. (<a href="https://doi.org/10.1007/s11263-024-02003-z" target="_blank">Paper</a>), (<a href="https://obrookes.github.io/panaf.github.io" target="_blank">Website</a>), (<a href="https://arxiv.org/abs/2401.13554" target="_blank">Arxiv PDF</a>), (<a href="https://doi.org/10.5523/bris.1h73erszj3ckn2qjwm4sqmr2wt" target="_blank">Dataset DOI:10.5523/bris.1h73erszj3ckn2qjwm4sqmr2wt</a>), (<a href="https://data.bris.ac.uk/datasets/1h73erszj3ckn2qjwm4sqmr2wt" target="_blank">Dataset Explorer</a>)

		  </DL>
		  </TABLE>

		  <TABLE>
			 <TD>  <img src="./PECOP.png", width="160", height="120" , ALIGN=LEFT/></a>
			  <TD>
			  <DL>
			  <DD> PECoP: Parameter Efficient Continual Pretraining for Action Quality Assessment. Amirhossein Dadashzadeh, Shuchao Duan, Alan Whone, Majid Mirmehdi. <em> WACV 2024 </em>
			        <DD> <a href="https://github.com/Plrbear/PECoP">Paper, Dataset and Code</a>
			   </DL>
			 </TABLE>

       <TABLE>
       <TD>  <img src="./cvpr23.jpg", width="160", height="85" , ALIGN=LEFT/></a>
       <TD>
        <DL>
           <DD>Use Your Head: Improving Long-Tail Video Recognition. T Perrett, S Sinha, T Burghardt, M Mirmehdi, D Damen. <em> CVPR 2023 </em>
             <DD> <a href="https://tobyperrett.github.io/lmr/">Paper and Code</a>
           </DL>
       </TABLE>

       <TABLE>
         <TD><img src="TranSOP.png" width="160", ALIGN=LEFT/>
         <TD>
        <DL>
       <DD>TranSOP: Transformer-based Multimodal Classification for Stroke Treatment Outcome Prediction. Z Samak, P. Clatworthy, M. Mirmehdi <em> 20th IEEE International Symposium on Biomedical Imaging. ISBI 2023 </em>
        <DD>  <a href="https://github.com/zakamas/TranSOP">Paper and Code</a>
        </DL>
        </TABLE>

       <TABLE>
       <TD>  <img src="./VISAPP2023.jpg", width="160", height="85" , ALIGN=LEFT/></a>
       <TD>
        <DL>
           <DD> Triple-stream Deep Metric Learning of Great Ape Behavioural Actions. O Brookes, M Mirmehdi, H Kuehl, T Burghardt.  <em> VISAPP 2023. </em>
             <DD> <a href="https://github.com/youshyee/DCLDet">Paper and Code</a>
           </DL>
       </TABLE>



       <TABLE>
       <TD>  <img src="./DCL2022.jpg", width="160", height="85" , ALIGN=LEFT/></a>
       <TD>
        <DL>
           <DD> Dynamic Curriculum Learning for Great Ape Detection in the Wild. X. Yang, T. Burghardt, M. Mirmehdi.  <em> International Journal of Computer Vision (IJCV), 2023. </em>
             <DD> <a href="https://github.com/youshyee/DCLDet">Paper and Code</a>
           </DL>
       </TABLE>

       <TABLE>
       <TD>  <img src="./ICIP23.jpg", width="160", height="85" , ALIGN=LEFT/></a>
       <TD>
        <DL>
           <DD>Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation.  Zeng, X Yang, D Smithard, M Mirmehdi, AM Gambaruto, T Burghardt.  <em>ICIP2023  </em>
             <DD> (<a href="https://arxiv.org/abs/2302.11325" target="_blank">Arxiv PDF</a>), (<a href="https://github.com/SimonZeng7108/Video-SwinUNet" target="_blank">GitHub</a>)
           </DL>
       </TABLE>

       <TABLE>
         <TD><img src="FEMA.jpg" width="160", ALIGN=LEFT/>
         <TD>
        <DL>
       <DD>FeMA: Feature matching auto-encoder for predicting ischaemic stroke evolution and treatment outcome. Z Samak, P. Clatworthy, M. Mirmehdi <em>Computerized Medical Imaging and Graphics, 2022 </em>
        <DD> <a href="https://www.sciencedirect.com/science/article/pii/S0895611122000623">Paper</a> |  <a href="https://github.com/zakamas/FeMA">Code</a>
        </DL>
        </TABLE>


       <TABLE>
       <TD>  <img src="./sphere.jpg", width="160", height="85" , ALIGN=LEFT/></a>
       <TD>
        <DL>
           <DD>Refining Action Boundaries for One-Stage Detection. H Wang, M Mirmehdi, D Damen,  T Perrett.  <em> AVSS 2022  </em>
             <DD> <a href="https://github.com/hanielwang/Refining_Boundary_Head">Paper and Code</a>
           </DL>


       </TABLE>
       <TABLE>
       <TD>  <img src="./sphere.jpg", width="160", height="85" , ALIGN=LEFT/></a>
       <TD>
        <DL>
           <DD>Personalised Energy Expenditure Estimation: A Visual Sensing Approach With Deep Learning. T Perrett, A Masullo, D Damen, T Burghardt, I Craddock, M Mirmehdi.  <em> JMIR Formative Research  2022  </em>
             <DD> <a href="https://formative.jmir.org/2022/9/e33606">Paper</a>
           </DL>
       </TABLE>

       <TABLE>
       <TD>  <img src="./wacvw22.jpg", width="160", height="85" , ALIGN=LEFT/></a>
       <TD>
        <DL>
           <DD>Small or Far Away? Exploiting Deep Super-Resolution and Altitude Data for Aerial Animal Surveillance. M Xue, T Greenslade, M. Mirmehdi and T. Burghardt.  </em> WACV Workshop (RWS) 2022.  </em>
             <DD> <a href="https://openaccess.thecvf.com/content/WACV2022W/RWS/papers/Xue_Small_or_Far_Away_Exploiting_Deep_Super-Resolution_and_Altitude_Data_WACVW_2022_paper.pdf">Paper</a>
           </DL>
       </TABLE>


       <TABLE>
       <TD>  <img src="./sphere.jpg", width="160", height="85" , ALIGN=LEFT/></a>
       <TD>
        <DL>
           <DD>No Need for a Lab: Towards Multi-Sensory Fusion for Ambient Assisted Living in Real-World Living Homes. A Masullo, TJ Perrett, D Damen, T Burghardt, M Mirmehdi.  <em> VISAPP 2021   </em>
             <DD> <a href="https://www.scitepress.org/Link.aspx?doi=10.5220/0010202903280337">Paper</a>
           </DL>
       </TABLE>

       <TABLE>
       <TD>  <img src="./bk2future.jpg", width="160", height="85" , ALIGN=LEFT/></a>
       <TD>
        <DL>
           <DD>Back to the Future: Cycle Encoding Prediction for Self-supervised Video Representation Learning. X. Yang, M. Mirmehdi and T. Burghardt,  <em> BMVC (2021).  </em>
             <DD> <a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_0399.html">Paper and Code</a>
           </DL>
       </TABLE>

       <TABLE>
       <TD>  <img src="./unsup3Dview.jpg", width="160", ALIGN=LEFT/></a>
       <TD>
        <DL>
           <DD>Unsupervised View-Invariant Human Posture Representation. F. Sardari, B. Ommer, and M. Mirmehdi,  <em> BMVC (2021).  </em>
             <DD> <a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_0634.html">Paper and Code</a>
           </DL>
       </TABLE>


       <TABLE>
       <TD>   <a href="https://tobyperrett.github.io/trxweb/"> <img src="./TRX.png", width="160", ALIGN=LEFT/></a>
       <TD>
        <DL>
           <DD>Temporal-Relational CrossTransformers for Few-Shot Action Recognition. T Perrett, A Masullo, T Burghardt, M Mirmehdi, D Damen.  <em> CVPR (2021).  </em>
             <DD> <a href="https://arxiv.org/abs/2101.06184">ArXiv</a> | <a href="https://tobyperrett.github.io/trxweb/">Project Webpage</a> | <a href="https://github.com/tobyperrett/trx">Code and Model</a>
           </DL>
       </TABLE>

      <TABLE>
      <TD> <a href="https://www.mdpi.com/1424-8220/20/9/2576"><img src="./negative_types.png" width="160", ALIGN=LEFT/></a>
      <TD>
       <DL>
      <DD>Person Re-ID by Fusion of Video Silhouettes and Wearable Signals for Home Monitoring Applications. A Masullo, T Burghardt, D Damen, T Perrett, M Mirmehdi. <em> Sensors 20(9) </em> 2020.
        <DD><a href="https://www.mdpi.com/1424-8220/20/9/2576">HTML</a> | <a href="https://www.mdpi.com/1424-8220/20/9/2576/pdf">PDF</a></p>
       </DL>
       </TABLE>


       <TABLE>
       <TD> <a href="https://tobyperrett.github.io/contextagnosticweb/"><img src="./ContextAgnostic.png" width="160", ALIGN=LEFT/></a>
       <TD>
        <DL>
       <DD>Meta-Learning with Context-Agnostic Initialisation. T Perrett, A Masullo, T Burghard, M Mirmehdi, D Damen. <em> ACCV </em> 2020.
          <DD> <a href="https://openaccess.thecvf.com/content/ACCV2020/html/Perrett_Meta-Learning_with_Context-Agnostic_Initialisations_ACCV_2020_paper.html">CVF</a> | <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Perrett_Meta-Learning_with_Context-Agnostic_Initialisations_ACCV_2020_paper.pdf">CVF PDF</a> | <a href="https://tobyperrett.github.io/contextagnosticweb/">Project Page</a> | <a href="https://youtu.be/SrksZ-motho">Talk Video</a></p>
        </DL>
        </TABLE>

       <TABLE>
       <TD> <a href="https://www.mdpi.com/1424-8220/20/18/5258"><img src="./sequencce_assessing11.png" width="160", ALIGN=LEFT/></a>
       <TD>
        <DL>
       <DD>VI-Net: View-Invariant Quality of Human Movement Assessment. F Sardari, A Paiement, S Hannuna, M Mirmehdi. <em> Sensors 20(18) </em> 2020.
         <DD> <a href="https://www.mdpi.com/1424-8220/20/18/5258">HTML</a> | <a href="https://www.mdpi.com/1424-8220/20/18/5258/pdf">PDF</a></p>
        </DL>
        </TABLE>


       <TABLE>
         <TD><img src="ThrombectomyNetwork.jpg" width="160", ALIGN=LEFT/>
         <TD>
        <DL>
       <DD>Prediction of Thrombectomy Functional Outcomes using Multimodal Data, Z Samak, P. Clatworthy, M. Mirmehdi <em> MIUA </em> 2020.
        <DD> <a href="https://link.springer.com/chapter/10.1007/978-3-030-52791-4_21">Springer Link</a>  | <a href="https://arxiv.org/abs/2005.13061v2"> arXive</a>
        </DL>
        </TABLE>



       <TABLE>
         <TD><img src="cvpm2019.png" width="160", ALIGN=LEFT/>
         <TD>
         <DL>
          <DD> Who Goes There? Exploiting Silhouettes and Wearable Signals for Subject Identification in Multi-Person Environments,
          A Masullo, T Burghardt, D Damen, T Perrett, M Mirmehdi. <em>2nd Int. Workshop on Computer Vision for Physiological Measurement (CVPM) at IEEE International Conference of Computer Vision (ICCVW),</em>  2019.
          <DD><a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/CVPM/Masullo_Who_Goes_There_Exploiting_Silhouettes_and_Wearable_Signals_for_Subject_ICCVW_2019_paper.pdf" target="_blank">CVF Version</a> | <a href="https://doi.org/10.5523/bris.1gt0wgkqgljn21jjgqoq8enprr" target="_blank">Dataset: SPHERE-Calorie</a>
        </DL>
        </TABLE>


      <TABLE>
          <TD><img src="cookingbehaviour.png" width=160  ALIGN=LEFT//>
          <TD>
          <DL>
            <DD> Analysing Cooking Behaviour in Home Settings: Towards Health Monitoring. K Yordanova, S Ludtke, S Whitehouse, F Kruger, A Paiement, M Mirmehdi, I Craddock, T Kirste. <em>Sensors</em>, 19(3), 646, 2019
            <DD> <a href="https://www.mdpi.com/1424-8220/19/3/646" target="_blank">Link to PDF</a>
        <!-- <p> Evaluation of cupboard door sensors for improving activity recognition in the kitchen. S. Whitehouse, K. Yordanova, S. Ludtke, A. Paiement and M. Mirmehdi, pp. 167-17, 2018 <p> https://ieeexplore.ieee.org/document/8480352 PDF -->
            <p><DD> What is cooking and Why? Behaviour Recognition during Unscripted Cooking Tasks for Health Monitoring. K Yordanova, S Whitehouse, A Paiement, M Mirmehdi, T Kirste, I Craddock. <em>PerCom</em> 2017 (Best work in progress paper award)
            <DD><a href="https://www.researchgate.net/publication/312056108_What's_cooking_and_Why_Behaviour_Recognition_during_Unscripted_Cooking_Tasks_for_Health_Monitoring">Link to PDF</a> | <a href="https://data.bris.ac.uk/data/dataset/raqa2qzai45z15b4n0za94toi"> Dataset: SPHERE Unscripted kitchen activities </a>
        </DL>
      </TABLE>



    <TABLE>
        <TD><img src="ApeOverview2019.jpg" width="160", height="85",  ALIGN=LEFT/>
        <TD>
        <DL>
          <DD> Great Ape Detection in Challenging Jungle Camera Trap Footage via Attention-Based Spatial and Temporal Feature Blending, X. Yang, M. Mirmehdi, T. Burghardt, <em> Computer Vision for Wildlife Conservation (CVWC) Workshop at
       IEEE International Conference of Computer Vision (ICCVW) </em>,  2019.
          <DD> <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/CVWC/Yang_Great_Ape_Detection_in_Challenging_Jungle_Camera_Trap_Footage_via_ICCVW_2019_paper.pdf">CVF Version</a> |
          <a href="https://arxiv.org/abs/1908.11240" target="_blank">Arxiv PDF</a> |
          <a href="http://people.cs.bris.ac.uk/~burghard/mpi2019.txt" target="_blank">Dataset PanAfrican2019 Video</a>  |
          <a href="https://doi.org/10.5523/bris.1v9op9lc6zi5g25kkwa5smb3vq" target="_blank">Dataset: PanAfrican2019 Annotations and Code</a>
        </DL>
      </TABLE>



    <TABLE>
        <TD><a href="http://people.cs.bris.ac.uk/~damen/ActionCompletion/"><img src="ActionCompletionDetectionIntro.png" width="160", ALIGN=LEFT/></a>
        <TD>
        <DL>
          <DD>Weakly-Supervised Completion Moment Detection using Temporal Attention. F Heidarivincheh, M Mirmehdi, D Damen. <em> ICCV Workshop on Human Behaviour Understanding</em>, 2019
          <DD> <a href="https://arxiv.org/abs/1910.09920">Arxiv PDF</a> | <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/HBU/Heidarivincheh_Weakly-Supervised_Completion_Moment_Detection_using_Temporal_Attention_ICCVW_2019_paper.pdf">CVF PDF</a>
          <p><DD>Action Completion: A Temporal Model for Moment Detection. F Heidarivincheh, M Mirmehdi, D Damen. <em>BMVC</em>, 2018
          <DD><a href="https://arxiv.org/abs/1805.06749">Arxiv PDF</a> | <a href="https://youtu.be/Hrxehk3Sutc">Video2018</a> | <a href="https://github.com/FarnooshHeidari/CompletionDetection">Dataset</a>
          <p><DD>Beyond Action Recognition: Action Completion in RGB-D Data. F Heidarivincheh, M Mirmehdi, D Damen. <em> BMVC</em>, 2016
          <DD><a href="http://people.cs.bris.ac.uk/~damen/ActionCompletion/ActionCompletion_BMVC2016.pdf">pdf</a> | <a href="http://people.cs.bris.ac.uk/~damen/ActionCompletion/ActionCompletion_BMVC2016_abstract.pdf">abstract</a> | <a href="https://youtu.be/iBdW-kVKMds">Video2016</a> | <a href="http://dx.doi.org/10.5523/bris.66qry08cv1fj1eunwxwob3fjz">Dataset: RGBD-Action-Completion-2016</a>
       </DL>
    </TABLE>


    <TABLE>
       <TD><img src="CalorieOld.png" width="160", ALIGN=LEFT//>
       <TD>
       <DL>
        <DD> CaloriNet: From silhouettes to calorie estimation in private environments. A Masullo, T Burghardt, D Damen, S Hannuna, V Ponce-Lopez, M Mirmehdi, <em> BMVC</em> 2018
        <DD> <a href="http://bmvc2018.org/contents/papers/0383.pdf">PDF</a>  | <a href="https://github.com/ale152/CaloriNet"> Code on Github</a>
        <p><DD> Energy expenditure estimation using visual and inertial sensors. L Tao, T Burghardt, M Mirmehdi, D Damen, A. Cooper, S. Hannuna, M Camplani, A. Paiement, I Craddock. <em> IET Computer Vision</em>, 12(1), 2018.
        <DD> <a href="https://digital-library.theiet.org/content/journals/10.1049/iet-cvi.2017.0112">OA PDF @IET</a>  | <a href="https://www.irc-sphere.ac.uk/work-package-2/calorie"> SPHERE Webpage</a>
       <!-- <p> Calorie Counter: RGB-Depth Visual Estimation of Energy Expenditure at Home. L Tao, T Burghardt, M Mirmehdi, D Damen, A. Cooper, S. Hannuna, M Camplani, A. Paiement, I Craddock. ACCV Workshops pp 239-251, 2016.
        <DD> <a href="https://link.springer.com/chapter/10.1007/978-3-319-54407-6_16">Link to PDF</a> |
          <a href="https://data.bris.ac.uk/data/dataset/1gt0wgkqgljn21jjgqoq8enprr">Dataset: SPHERE-Calorie</a>  | <a href="https://www.irc-sphere.ac.uk/work-package-2/calorie"> SPHERE Webpage</a> -->
     </DL>
   </TABLE>


    <TABLE>
       <TD><img src="TwoKinectSynch.gif" width="160", ALIGN=LEFT/>
       <TD>
       <DL>
       <DD> 3D Data Acquisition and Registration using Two Opposing Kinects. V Soleimani, M Mirmehdi, D Damen, S Hannuna, M Camplani, <em>4th International Conference on 3D Vision (3DV)</em>, 128-137, 2016
       <DD> <a href="https://ieeexplore.ieee.org/abstract/document/7785085"> PDF@ieeexplore </a> | <a href="https://github.com/BristolVisualPFT/3D_Data_Acquisition_Registration_Using_Kinects/tree/master/Double_opposing_Kinects"> Code</a>
       </DL>
    </TABLE>


    <TABLE>
       <TD><img src="DSKCF.png" width="160", ALIGN=LEFT/>
       <TD>
       <DL>
        <DD>Real-time RGB-D Tracking with Depth Scaling Kernelised Correlation Filters and Occlusion Handling. M Camplani, S Hannuna, M Mirmehdi, D Damen, L Tao, T Burghardt and A Paiment. British Machine Vision Conference (BMVC), Sep 2015.
        <DD>  <a href="http://bmvc2015.swansea.ac.uk/proceedings/papers/paper145/paper145.pdf">PDF</a> |
              <a href="https://www.youtube.com/watch?v=yhT2PdN9BTw&amp;feature=youtu.be">Video 1</a> |
              <a href="https://www.youtube.com/watch?v=YoFMf2iARzA&amp;feature=youtu.be">Video 2</a> |
              <a href="http://data.bris.ac.uk/data/dataset/16vbnj3im1ygi1sh0yd0mt4lp0">Code on Github</a>
        </DL>
     </TABLE>


    <TABLE>
        <TD><img src="Quality.png" width="160", ALIGN=LEFT/ >
          <TD>
          <DL>
          <DD>Online quality assessment of human movement from skeleton data. A Paiment, L Tao, S Hannuna, M Camplani, D Damen and M Mirmehdi. British Machine Vision Conference (BMVC), Sep 2014.
          <DD><a href="http://www.cs.bris.ac.uk/home/palement/articles/bmvc2014.pdf">PDF</a> | <a href="http://data.bris.ac.uk/data/dataset/bgresiy3olk41nilo7k6xpkqf">Dataset</a> | <a href="http://www.cs.bris.ac.uk/home/palement/datasets/code_movement_quality.zip">Code</a></p>
       </DL>
    </TABLE>


     <h3><a href="http://www.amazon.co.uk/HANDBOOK-TEXTURE-ANALYSIS-MIRMEHDI-MAJID/dp/1848161158/ref=sr_1_1?ie=UTF8&s=books&qid=1245248607&sr=1-1" target="new"> Handbook of Texture Analysis </a> </h3>
     <TABLE>
        <TD><a href="http://www.amazon.co.uk/HANDBOOK-TEXTURE-ANALYSIS-MIRMEHDI-MAJID/dp/1848161158/ref=sr_1_1?ie=UTF8&s=books&qid=1245248607&sr=1-1" target="new"> <img alt="Handbook of Texture Analysis" border="0" src="frontcoverTexture.jpg" width="120" align="right"></a>
        <TD>
        <DL>
         <DD> Collection of chapters on many aspects of texture analysis (pre-deep learning era!)
         <DD> Hardcover: 424 pages - Publisher: Imperial College Press (Dec. 2008) - ISBN-13: 978-1848161153
         <DD> <a href="http://www.cs.bris.ac.uk/Publications/pub_master.jsp?id=2000853">Sample chapter: A Galaxy of Texture Features</a>
      </DL>
      </TABLE>



 <hr size="1">
<p><em>Last Updated April 2024</em> </p>

</TD>
</TABLE>

</body>
</html>


</body>
</html>
